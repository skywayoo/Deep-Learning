{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Merge\n",
    "from keras.layers import Convolution1D ,AveragePooling1D\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=open(\"Downloads/Gun_Point_TRAIN\")\n",
    "test=open(\"Downloads/Gun_Point_TEST\")\n",
    "train = train.read().split()\n",
    "test = test.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 151)\n",
      "(150, 151)\n"
     ]
    }
   ],
   "source": [
    "train = np.array_split(np.array(train),50)\n",
    "test = np.array_split(np.array(test),150)\n",
    "print(np.shape(train))\n",
    "print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the label of train and test\n",
    "def Get_label(data):\n",
    "    label=[]\n",
    "    for i in range(0,len(data)):\n",
    "        label.append(float(data[i][0]))\n",
    "    return label\n",
    "y_train = Get_label(train)\n",
    "y_test= Get_label(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the value of train and test\n",
    "def Get_value(data):\n",
    "    value=[]\n",
    "    for i in range(0,len(data)):\n",
    "        value.append(data[i][1:151])\n",
    "    return value\n",
    "X_train=Get_value(train)\n",
    "X_test=Get_value(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (50, 150, 1))\n",
      "('X_test shape:', (150, 150, 1))\n"
     ]
    }
   ],
   "source": [
    "#set the series \n",
    "length=150\n",
    "X_train=np.asarray(X_train)\n",
    "X_test=np.asarray(X_test)\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = np.asarray(y_train)\n",
    "y_train= y_train.astype(int)\n",
    "y_test = np.asarray(y_test)\n",
    "y_test= y_test.astype(int)\n",
    "y_test=y_test-1\n",
    "y_train=y_train-1\n",
    "X_train=X_train.reshape(50,150,1)\n",
    "X_test=X_test.reshape(150,150,1)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_output(labels):\n",
    "    return to_categorical(labels, nb_classes=2)\n",
    "y_train = process_output(y_train)\n",
    "y_test = process_output(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution1d_1 (Convolution1D)  (None, 143, 50)       450         convolution1d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 143, 50)       0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "averagepooling1d_1 (AveragePoolin(None, 71, 50)        0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 64, 100)       40100       averagepooling1d_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 64, 100)       0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "averagepooling1d_2 (AveragePoolin(None, 32, 100)       0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 3200)          0           averagepooling1d_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 500)           1600500     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             1002        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_3 (Convolution1D)  (None, 143, 50)       450         convolution1d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 143, 50)       0           convolution1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "averagepooling1d_3 (AveragePoolin(None, 71, 50)        0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 64, 100)       40100       averagepooling1d_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 64, 100)       0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "averagepooling1d_4 (AveragePoolin(None, 32, 100)       0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 3200)          0           averagepooling1d_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 500)           1600500     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             1002        dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 2)             10          merge_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3284114\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "#first model\n",
    "model = Sequential()\n",
    "#1st convolution\n",
    "model.add(Convolution1D(50,8,input_shape=(150, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling1D(pool_length=2))\n",
    "#2st convolution\n",
    "model.add(Convolution1D(100,8))\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "#fulley connected layer\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dense(2))\n",
    "#second model\n",
    "model2 = Sequential()\n",
    "#1st convolution\n",
    "model2.add(Convolution1D(50,8,input_shape=(150, 1)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(AveragePooling1D(pool_length=2))\n",
    "#2st convolution\n",
    "model2.add(Convolution1D(100,8))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(AveragePooling1D(pool_length=2))\n",
    "model2.add(Flatten())\n",
    "#fulley connected layer\n",
    "model2.add(Dense(500,activation=\"relu\"))\n",
    "model2.add(Dense(2))\n",
    "#merge model\n",
    "merged = Merge([model, model2], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "#keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "50/50 [==============================] - 2s - loss: 8.1560 - acc: 0.5200\n",
      "Epoch 2/2\n",
      "50/50 [==============================] - 2s - loss: 7.9390 - acc: 0.4800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa829a4bbd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input one data is work...\n",
    "batch_size = 50\n",
    "nb_epoch = 2\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 2 arrays: [array([[[-0.64788538],\n        [-0.64199156],\n        [-0.63818634],\n        ..., \n        [-0.64042872],\n        [-0.63866574],\n        [-0.63865721]],\n\n       [[-0.64442658],\n        [-0.64540094],...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b08f18ab7a2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m model.fit([X_train,X_test], y_train,\n\u001b[0;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m           \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m           )\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    992\u001b[0m                                                            \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m                                                            batch_size=batch_size)\n\u001b[0m\u001b[0;32m    995\u001b[0m         \u001b[1;31m# prepare validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[0;32m    919\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                                    exception_prefix='model input')\n\u001b[0m\u001b[0;32m    922\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[0;32m    923\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[0;32m     47\u001b[0m                                 \u001b[1;34m'the following list of '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                                 \u001b[1;34m' arrays: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                                 '...')\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 2 arrays: [array([[[-0.64788538],\n        [-0.64199156],\n        [-0.63818634],\n        ..., \n        [-0.64042872],\n        [-0.63866574],\n        [-0.63865721]],\n\n       [[-0.64442658],\n        [-0.64540094],..."
     ]
    }
   ],
   "source": [
    "#input two data.  GET ERROR\n",
    "#X_train to CNN1 \n",
    "#X_test to CNN2\n",
    "batch_size = 50\n",
    "nb_epoch = 2\n",
    "model.fit([X_train,X_test], y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
